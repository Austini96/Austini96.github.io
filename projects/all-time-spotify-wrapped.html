<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GK4H1HF9K6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-GK4H1HF9K6');
    </script>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>All-Time Spotify Wrapped - Austin Lee</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="project-styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-brand">Austin Lee</div>
        <div class="nav-toggle" id="navToggle">
            <span></span>
            <span></span>
            <span></span>
        </div>
        <ul class="nav-menu">
            <li><a href="../index.html#home">Home</a></li>
            <li><a href="../index.html#about">About</a></li>
            <li><a href="../index.html#skills">Skills</a></li>
            <li><a href="../index.html#experience">Experience</a></li>
            <li><a href="../index.html#education">Education</a></li>
            <li><a href="../index.html#projects">Projects</a></li>
            <li><a href="../index.html#contact">Contact</a></li>
        </ul>
    </nav>

    <!-- Project Detail Section -->
    <section class="project-detail">
        <div class="container">
            <a href="../index.html#projects" class="back-link"><i class="fas fa-arrow-left"></i> Back to Projects</a>
            
            <div class="project-header">
                <h1>All-Time Spotify Wrapped</h1>
                <div class="project-meta">
                    <span><i class="fas fa-calendar"></i> 2025</span>
                    <span><i class="fas fa-tag"></i> Data Engineering</span>
                </div>
            </div>

            <div class="toc">
                <h2>Table of Contents</h2>
                <ul>
                    <li><a href="#overview">1. Overview</a>
                        <ul>
                            <li><a href="#objective">1-1. Objective</a></li>
                            <li><a href="#key-features">1-2. Key Features</a></li>
                        </ul>
                    </li>
                    <li><a href="#architecture">2. Architecture</a>
                        <ul>
                            <li><a href="#tools-technologies">2-1. Tools & Technologies</a></li>
                            <li><a href="#data-flow">2-2. Data Flow</a></li>
                        </ul>
                    </li>
                    <li><a href="#dashboard-insights">3. Dashboard & Insights</a></li>
                    <li><a href="#things-I-learned">4. Things I Learned</a></li>
                    <li><a href="#future-improvements">5. Future Improvements</a></li>
                </ul>
            </div>

            <div class="project-content">
                <div id="overview" class="project-section">
                    <h2>1. Overview</h2>
                    
                    <p>All-Time Spotify Wrapped project aims to create a comprehensive data pipeline that extracts, transforms, and visualizes a user's complete Spotify listening history using modern data engineering tools and practices. This project recreates <strong>Spotify Wrapped</strong> that can be viewed at any time of year with richer insights than the official version.</p>

                    <div class="hypothesis-box">
                        <p><strong>Reference:</strong> This project is built upon concepts from <a href="https://github.com/calbergs/spotify-api.git" target="_blank" class="paper-link">spotify-api</a> by calbergs and is developed further into incorporating DuckDB for OLAP database as well as OpenLineage and Marquez for complete column level data lineage.</p>
                    </div>

                    <div id="objective">
                        <h3>1-1. Objective</h3>
                        <p>The main goal of this project is to extract all of my song listening history data and continuously extract the Spotify usage activity through Spotify API. I will deep dive into my complete song listening history to analyze top artists, tracks, playlists, and listening patterns.</p>
                    </div>

                    <div id="key-features">
                        <h3>1-2. Key Features</h3>
                        <ul>
                            <li><strong>Real-time Tracking:</strong> Hourly API calls during active hours (0-6, 14-23 UTC)</li>
                            <li><strong>Historical Analysis:</strong> Integrates 2+ years of extended streaming history (89K+ plays)</li>
                            <li><strong>Usage Pattern Discovery:</strong> Discover my usage patterns and habits over time</li>
                            <li><strong>Data Lineage:</strong> Complete column-level lineage tracking with OpenLineage + Marquez</li>
                            <li><strong>Cost Efficient:</strong> 100% open-source tools, hosted locally</li>
                        </ul>
                    </div>
                </div>

                <div id="architecture" class="project-section">
                    <h2>2. Architecture</h2>
                    
                    <div class="project-architecture">
                        <figure>
                            <img src="project images/Architecture.png" alt="Architecture" width="100%"/>
                        </figure>
                    </div>

                    <p>One of the main achievements I wanted out of this project was to learn different tools and technologies by building a complete data pipeline. Since my work environment is primarily in Azure Synapse Analytics, I wanted to use well known open-source tools and technologies such as Apache Airflow, dbt, DuckDB, and Docker to learn what they are capable of and why they are widely used in the industry. Also, as part of data governance, I was interested in implementing an end-to-end column level data lineage, and this project was the perfect opportunity to try implementing it using OpenLineage and Marquez.</p>

                    <div id="data-flow">
                        <h3>2-2. Data Flow</h3>

                        <p>Below is the holistic view of the data pipeline orchestrated by Apache Airflow.</p>

                        <div class="airflow-dag-image">
                            <figure>
                                <img src="project images/Airflow DAG.png" alt="Airflow DAG" width="100%"/>
                            </figure>
                        </div>
                        
                        <p>The main components of the data pipeline are <strong>Extraction</strong>, <strong>Loading</strong>, <strong>Transformation</strong>, <strong>Lineage Tracking</strong>, <strong>Data Sync</strong>, <strong>Quality & Documentation</strong>, and <strong>Slack Notifications</strong>.</p>
                        
                        <h4>1. Extraction</h4>

                        <p>The extraction process has two main parts: <strong>API Extraction</strong> and <strong>Extended History</strong>.</p>
                        
                        <ul>
                            <li><strong>API Extraction</strong> is responsible for fetching the recently played tracks and their audio features. The limitation of this API extraction is that it only returns 50 most recent tracks, and this is where the extended history comes into play.</li>
                            <li><strong>Extended History</strong> is responsible for fetching the historical streaming data from Spotify. Unlike the API extraction, this I need to manually request the extended history data from Spotify. Once I receive the data in JSON format, I need to parse the data and store it as a CSV file.</li>
                        </ul>

                        <h4>2. Loading</h4>

                        <p>After ingesting the data from the extraction process, they are loaded into DuckDB for further analysis.</p>

                        <h4>3. Transformation (dbt)</h4>

                        <p>dbt is used to transform the loaded data into a star schema for analysis. What I focused here was to have a proper dimensional modeling implemented and have a medallion structure. The tables are organized into three different layers: <strong>Staging Layer</strong>, <strong>Marts Layer</strong>, and <strong>Analytics Layer</strong>. Tables in staging layer is essentially silver tables where the raw data is loaded and cleaned up. The gold tables are in the mart layer, having fact tables and dimension tables from the silver tables. The analytics layer is the layer where different tables for analytics are created.</p>

                        <h4>4. Lineage Tracking</h4>

                        <p>I implemented OpenLineage to track column level lineage for all dbt models. One challenge I faced here was integrating OpenLineage with dbt. The official documentation suggests that running <code>dbt-ol run</code> instead of <code>dbt run</code> automatically emits column level lineage. However, I found that this approach was not working as expected, so I ended up making a separate script to manually emit the lineage events. This was not what I have aimed for in the first place, but it was the best solution I could come up with at the time. To visualize the lineage, I used the open-source tool Marquez. I was able to visualize the column level lineage for all dbt models through Marquez, but one of the major goals for my next step would be to implement the automation of the lineage tracking process.</p>
                        
                        <div class="marquez-lineage-image">
                            <figure>
                                <img src="project images/Marquez Lineage.png" alt="Marquez Data Lineage" width="100%"/>
                            </figure>
                        </div><br>

                        <h4>5. Data Sync</h4>

                        <p>This process is responsible for syncing the data from DuckDB to PostgreSQL. The reason I did this was because I ran into an issue where I was not able to connect Metabaseto DuckDB directly. I read the documentation and found that I could connect Metabase with DuckDB using Metabase DuckDB Driver. However, I ran into a configuration issue with it and decided to work my way around it by syncing the data to PostgresSQL and then connecting Metabase to PostgresSQL directly. One other major goal that I want to tackle for my next step is to resolve this integration issue and connect Metabase to DuckDB directly so that I don't have to sync the data to PostgresSQL unnecessarily.</p>

                        <h4>6. Quality & Documentation</h4>

                        <p>For the dbt models, I wanted to test out the dbt test and documentation features, so I added some light tests and documentation to the models for the data governance purposes. For the next step, I want to implement more comprehensive data quality checks that cross reference different columns and tables to ensure the data is accurate and consistent. I want to also be able to create a data quality dashboard if I can get the data quality testing data from dbt.</p>

                        <h4>7. Slack Notifications</h4>

                        <div class="project-image-row">
                            <figure>
                                <img src="project images/Slack Notification.png" alt="Slack Notification" width="100%"/>
                            </figure>
                        </div>

                        <p>To ensure that I am notified for any pipeline failures or retries, I integrated Slack into the pipeline to send notifications to my Slack workspace. The message tells me the task that failed, the error message, and the retry count. I thought this was a good way to ensure even when I am out of home to check on the pipeline, I can still be notified and take action accordingly.</p>
                    </div>
                </div>

                <div id="dashboard-insights" class="project-section">
                    <h2>3. Dashboard & Insights</h2>
                    
                    <h3>Metabase Dashboard</h3>
                    <p>Below are visualizations from the Metabase dashboard showcasing comprehensive Spotify listening analytics:</p>
                    <div class="project-image-row">
                        <figure>
                            <img src="project images/Metabase1.png" alt="Metabase Dashboard 1" width="100%"/>
                            <img src="project images/Metabase2.png" alt="Metabase Dashboard 2" width="100%"/>
                            <img src="project images/Metabase3.png" alt="Metabase Dashboard 3" width="100%"/>
                        </figure>
                    </div>
                </div>

                <div id="things-I-learned" class="project-section">
                    <h2>4. Things I Learned</h2>
                    <p>Throughout the course of this project, I learned a lot about the foundations of data engineering and data pipeline orchestration. This was a great opportunity to learn about modern data engineering technique, including how to use Apache Airflow to orchestrate the data pipeline and how to use dbt to transform the data. I read a lot of articles about how good DuckDB is, so having a chance to have the experience of loading the data in DuckDB gave me a better understanding of its capabilities. One thing that I'm really satisfied about this project is that I got to use OpenLineage to track the column level lineage of the dataand visualize it through Marquez. I have been wanting to learn how to incorporate OpenLineage specifically to retrieve column level lineage from my work environment, so this project was a great starting point on that journey.</p>
                </div>

                <div id="future-improvements" class="project-section">
                    <h2>5. Future Improvements</h2>
                    <p>I think there is a lot of room for improvement for this project. As of now, I'm thinking about focusing on the following areas for my next steps:</p>
                    <ul>
                        <li>Implementing the automation of the lineage tracking process</li>
                        <li>Implementing more comprehensive data quality checks that cross reference different columns and tables to ensure the data is accurate and consistent</li>
                        <li>Creating a data quality dashboard if I can get the data quality testing data from dbt</li>
                        <li>Resolving the integration issue and connecting Metabase to DuckDB directly so that I don't have to sync the data to PostgresSQL unnecessarily</li>
                    </ul>
                </div>

                <div class="hypothesis-box">
                    <h3>Acknowledgments</h3>
                    <ul>
                        <li>calbergs for the basic guideline of the project</li>
                        <li>Spotify for providing comprehensive Web API</li>
                        <li>Open-source communities behind Airflow, dbt, DuckDB, OpenLineage, and Marquez</li>
                        <li>Inspired by Spotify Wrapped</li>
                    </ul>
                </div>

                <div class="hypothesis-box">
                    <h3>📝 License</h3>
                    <p>This is a personal project for educational and analytical purposes. Spotify API usage complies with <a href="https://developer.spotify.com/terms" target="_blank" class="paper-link">Spotify's Developer Terms</a>.</p>
                </div>
            </div>
        </div>
    </section>

    <script src="../script.js"></script>
</body>
</html>
